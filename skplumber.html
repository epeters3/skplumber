<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>skplumber.skplumber API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>skplumber.skplumber</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import typing as t
from typing import NamedTuple
from time import time

import pandas as pd
from sklearn.utils import shuffle

from skplumber.consts import ProblemType
from skplumber.samplers.sampler import PipelineSampler, SamplerState
from skplumber.samplers.straight import StraightPipelineSampler
from skplumber.primitives.primitive import Primitive
from skplumber.primitives.sk_primitives.classifiers import classifiers
from skplumber.primitives.sk_primitives.regressors import regressors
from skplumber.primitives.sk_primitives.transformers import transformers
from skplumber.metrics import default_metrics, metrics, Metric
from skplumber.utils import logger
from skplumber.evaluators import make_train_test_evaluator
from skplumber.tuners.ga import ga_tune
from skplumber.progress import EVProgress


class SKPlumberFitState:
    def __init__(self, budget: int, metric: Metric) -&gt; None:
        self.starttime = time()
        self.endbytime = self.starttime + budget
        self.best_pipeline_min_tune_time = 0.0
        self.best_score = metric.worst_value


class SearchResult(NamedTuple):
    # total train time in seconds
    time: float
    # total number of pipelines the sampler tried
    n_sample_iters: int
    # total number of pipelines the hyperparameter tuner tried
    n_tune_iters: int
    # the best score SKPlumber was able to find
    best_score: float


class SKPlumber:

    _models_map: t.Dict[ProblemType, t.List[t.Type[Primitive]]] = {
        ProblemType.CLASSIFICATION: list(classifiers.values()),
        ProblemType.REGRESSION: list(regressors.values()),
    }

    def __init__(
        self,
        problem: str,
        budget: int,
        *,
        metric: str = None,
        sampler: PipelineSampler = None,
        evaluator: t.Callable = None,
        pipeline_timeout: t.Optional[int] = None,
        exit_on_pipeline_error: bool = True,
        callback: t.Optional[t.Callable] = None,
        # TODO: make True by default. Requires being able to control the
        # amount of time it runs for (for tests and the budget)
        block_size: int = 10,
        tune: bool = False,
        tuning_mult_factor: int = 10,
        log_level: str = &#34;INFO&#34;,
    ) -&gt; None:
        &#34;&#34;&#34;
        Parameters
        ----------
        problem : str
            The type of problem you want to train this dataset for e.g.
            &#34;classification&#34;. See the values of the `skplumber.consts.ProblemType`
            enum for a list of valid options.
        budget : int
            How much time in seconds `fit` is allowed to search for a good solution.
        metric : Metric, optional
            The type of metric you want to score the pipeline predictions with.
            See the keys of the `skplumber.metrics.metrics` dict for a list of
            valid options. If `None`, a default metric will be used.
        sampler : PipelineSampler, optional
            An instance of a class inheriting from
            `skplumber.samplers.sampler.PipelineSampler`. Used to decide what
            types of pipelines to sample and try out on the problem. If `None`,
            the `skplumber.samplers.straight.StraightPipelineSampler` will be
            used with default arguments.
        evaluator : function, optional
            The evaluation strategy to use to fit and score a pipeline to determine
            its performance. Must be a function with signature:
            ```
            f(
                pipeline: Pipeline, X: pd.DataFrame, y: pd.Series, metric: Metric
            ) -&gt; float:
            ```,
            meaning it must accept a `skplumber.Pipeline` object, dataset `X` with
            corresponding target vector `y`, and an instantiation of a class
            inheriting from `skplumber.metrics.Metric`. Should fit on the dataset
            and compute and return its performance on that dataset. For basic k-fold
            cross validation or train/test split strategies, see
            `skplumber.evaluators.make_kfold_evaluator` or
            `skplumber.evaluators.make_train_test_evaluator`.
            If no evaluator is provided, a default train/test split evaluation strategy
            will be used. `evaluator` will be used during both the sampling and
            hyperparameter tuning stages, if `tune==True`.
        pipeline_timeout : int, optional
            If supplied, the maximum number of seconds to spend evaluating any
            one pipeline. If a sampled pipeline takes longer than this to evaluate,
            it will be skipped.
        exit_on_pipeline_error : bool, optional
            Whether to exit if a specific pipeline errors out while training. If
            `False`, the pipeline will just be skipped and `SKPlumber` will continue.
        callback : function, optional
            Optional callback function. Will be called after each sampled pipeline
            is evaluated. Should have the function signature
            `callback(state: SamplerState) -&gt; bool`. If `callback` returns `True`, the
            sampling or hyperparameter optimization will end prematurely. `state`
            is a named tuple containing these members:
                - pipeline: The pipeline fitted in the previous iteration.
                - score: The score of `pipeline`.
                - train_time: The number of seconds it took to train and
                  evaluate `pipeline`.
                - n_iters: The number of iterations completed so far.
        block_size : int, optional
            The block size to take extrema from when using the block maxima approach
            to calculate return times in the Generalized Extreme Value (GEV)
            distribution fit to the pipeline sample results as the sampler progresses.
        tune : bool, optional
            Whether to perform hyperparameter optimization on the best found pipeline.
        tuning_mult_factor : int, optional
            Each hyperparameter tuning generation will have a population size equal to
            the number of hyerparameters on the pipeline being optimized times this
            value.
        log_level : {&#39;INFO&#39;, &#39;DEBUG&#39;, &#39;NOTEST&#39;, &#39;WARNING&#39;, &#39;ERROR&#39;, &#39;CRITICAL&#39;}, optional
            Log level SKPlumber should use while running. Defaults to `&#34;INFO&#34;`.
        &#34;&#34;&#34;
        logger.setLevel(log_level)

        # Validate inputs

        self.problem_type = ProblemType(problem)

        valid_metric_names = [
            name
            for name, met in metrics.items()
            if met.problem_type == self.problem_type
        ]

        if metric is not None and metric not in valid_metric_names:
            raise ValueError(f&#34;metric is invalid, must be one of {valid_metric_names}&#34;)

        # Set defaults

        if metric is None:
            self.metric = default_metrics[self.problem_type]
        else:
            self.metric = metrics[metric]

        if sampler is None:
            self.sampler: PipelineSampler = StraightPipelineSampler()
        else:
            self.sampler = sampler

        if evaluator is None:
            self.evaluator = make_train_test_evaluator()
        else:
            self.evaluator = evaluator

        # Set other members

        self.budget = budget
        self.pipeline_timeout = pipeline_timeout
        self.tune = tune
        self.progress = EVProgress(block_size, self.metric.opt_dir)
        self.is_fitted = False
        self.tuning_mult_factor = tuning_mult_factor
        self.exit_on_pipeline_error = exit_on_pipeline_error

        self.sampler_cbs = [self._sampler_cb]
        if callback:
            self.sampler_cbs.append(callback)

    def fit(self, X: pd.DataFrame, y: pd.Series) -&gt; SearchResult:
        &#34;&#34;&#34;
        The main runtime method of the package. Given a dataset, problem type,
        and sampling strategy, it tries to find, in a limited amount of time,
        the best performing pipeline it can.

        Parameters
        ----------
        X : pandas.DataFrame
            The features of your dataset.
        y : pandas.Series
            The target vector of your dataset. The indices of `X` and `y`
            should match up.

        Returns
        -------
        result : SearchResult
            A named tuple containing data about how the fit process went.
        &#34;&#34;&#34;

        # Initialize

        if len(X.index) != y.size:
            raise ValueError(f&#34;X and y must have the same number of instances&#34;)

        # A little encapsulation to make this `fit` method&#39;s code less huge.
        self.state = SKPlumberFitState(self.budget, self.metric)

        # Run

        self.progress.start()
        best_pipeline, best_score, n_sample_iters = self.sampler.run(
            X,
            y,
            models=self._models_map[self.problem_type],
            transformers=list(transformers.values()),
            problem_type=self.problem_type,
            metric=self.metric,
            evaluator=self.evaluator,
            pipeline_timeout=self.pipeline_timeout,
            callback=self.sampler_cbs,
            exit_on_pipeline_error=self.exit_on_pipeline_error,
        )
        self.best_pipeline = best_pipeline
        self.state.best_score = best_score

        logger.info(f&#34;found best validation score of {best_score}&#34;)
        logger.info(&#34;best pipeline:&#34;)
        logger.info(self.best_pipeline)

        if self.tune:
            logger.info(
                &#34;now performing hyperparameter tuning on best found pipeline...&#34;
            )
            best_tuning_score, best_tuning_params, n_tune_iters = ga_tune(
                self.best_pipeline,
                X,
                y,
                self.evaluator,
                self.metric,
                self.exit_on_pipeline_error,
                population_size=(
                    self.best_pipeline.num_params * self.tuning_mult_factor
                ),
                callback=self._tuner_callback,
            )
            if self.metric.is_better_than(best_tuning_score, self.state.best_score):
                # The hyperparameter tuning was able to find an
                # improvement.
                self.state.best_score = best_tuning_score
                self.best_pipeline.set_params(best_tuning_params)
        else:
            n_tune_iters = 0

        # Now that we have the &#34;best&#34; model, train it on
        # the full dataset so it can see as much of the
        # dataset&#39;s distribution as possible in an effort
        # to be more ready for the wild.
        self.best_pipeline.fit(*shuffle(X, y))

        logger.info(
            &#34;finished. total execution time: &#34;
            f&#34;{time() - self.state.starttime:.2f} seconds.&#34;
        )
        logger.info(f&#34;final best score found: {self.state.best_score}&#34;)

        result = SearchResult(
            time() - self.state.starttime,
            n_sample_iters,
            n_tune_iters,
            self.state.best_score,
        )

        # Fitting completed successfully
        self.is_fitted = True
        return result

    def predict(self, X: pd.DataFrame) -&gt; pd.Series:
        &#34;&#34;&#34;
        Makes a prediction for each instance in `X`, returning the predictions.
        &#34;&#34;&#34;
        if not self.is_fitted:
            raise ValueError(
                &#34;`SKPlumber.fit` must be called and finish &#34;
                &#34;before `predict` can be called.&#34;
            )
        return self.best_pipeline.predict(X)

    def _sampler_cb(self, sampler_state: SamplerState) -&gt; bool:
        # Decide how much time is left available to us in
        # the sampling phase.
        if self.tune:
            # We want to leave enough time in the budget to be able
            # to complete at least one generation of hyperparameter tuning.
            if self.metric.is_better_than(sampler_state.score, self.state.best_score):
                self.state.best_score = sampler_state.score
                # An estimate of how long it will take to complete one
                # generation of hyperparameter tuning on this current
                # best pipeline.
                self.state.best_pipeline_min_tune_time = (
                    sampler_state.train_time
                    * sampler_state.pipeline.num_params
                    * self.tuning_mult_factor
                )
            sampling_endtime = (
                self.state.endbytime - self.state.best_pipeline_min_tune_time
            )
        else:
            sampling_endtime = self.state.endbytime

        now = time()
        logger.info(f&#34;{sampling_endtime - now:.2f} seconds left in sampling budget&#34;)

        # Logic for tracking sampler progress and exiting when the cost
        # of finding a new best score is too great.
        self.progress.observe(sampler_state.score)

        exit_early = False
        if now &gt; sampling_endtime:

            exit_early = True

        elif self.progress.can_report:

            logger.info(
                f&#34;estimated time to new best: {self.progress.return_time:.2f} seconds&#34;
            )
            if now + self.progress.return_time &gt; sampling_endtime:  # type: ignore
                exit_early = True

        if exit_early:
            logger.info(
                &#34;not enough time is left in the budget to find a new &#34;
                &#34;best score, so no more sampling will be done&#34;
            )

        return exit_early

    def _tuner_callback(self, tuner_state: dict) -&gt; bool:
        now = time()
        logger.info(
            f&#34;candidate pipeline in generation {tuner_state[&#39;nit&#39;]} finished. &#34;
            f&#34;{self.state.endbytime - now:.2f} seconds left in budget.&#34;
        )
        logger.info(f&#34;best score found so far: {tuner_state[&#39;fun&#39;]}&#34;)
        logger.info(
            f&#34;best hyperparameter config found so far: {tuner_state[&#39;kwargs_opt&#39;]}&#34;
        )
        # We need to quit early if our time budget is used up.
        return True if time() &gt; self.state.endbytime else False</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="skplumber.skplumber.SKPlumber"><code class="flex name class">
<span>class <span class="ident">SKPlumber</span></span>
<span>(</span><span>problem: str, budget: int, *, metric: str = None, sampler: <a title="skplumber.samplers.sampler.PipelineSampler" href="samplers/sampler.html#skplumber.samplers.sampler.PipelineSampler">PipelineSampler</a> = None, evaluator: Callable = None, pipeline_timeout: Union[int, NoneType] = None, exit_on_pipeline_error: bool = True, callback: Union[Callable, NoneType] = None, block_size: int = 10, tune: bool = False, tuning_mult_factor: int = 10, log_level: str = 'INFO')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>problem</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of problem you want to train this dataset for e.g.
"classification". See the values of the <code><a title="skplumber.consts.ProblemType" href="consts.html#skplumber.consts.ProblemType">ProblemType</a></code>
enum for a list of valid options.</dd>
<dt><strong><code>budget</code></strong> :&ensp;<code>int</code></dt>
<dd>How much time in seconds <code>fit</code> is allowed to search for a good solution.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>Metric</code>, optional</dt>
<dd>The type of metric you want to score the pipeline predictions with.
See the keys of the <code>skplumber.metrics.metrics</code> dict for a list of
valid options. If <code>None</code>, a default metric will be used.</dd>
<dt><strong><code>sampler</code></strong> :&ensp;<code>PipelineSampler</code>, optional</dt>
<dd>An instance of a class inheriting from
<code><a title="skplumber.samplers.sampler.PipelineSampler" href="samplers/sampler.html#skplumber.samplers.sampler.PipelineSampler">PipelineSampler</a></code>. Used to decide what
types of pipelines to sample and try out on the problem. If <code>None</code>,
the <code><a title="skplumber.samplers.straight.StraightPipelineSampler" href="samplers/straight.html#skplumber.samplers.straight.StraightPipelineSampler">StraightPipelineSampler</a></code> will be
used with default arguments.</dd>
<dt><strong><code>evaluator</code></strong> :&ensp;<code>function</code>, optional</dt>
<dd>The evaluation strategy to use to fit and score a pipeline to determine
its performance. Must be a function with signature:
<code>f(
pipeline: Pipeline, X: pd.DataFrame, y: pd.Series, metric: Metric
) -&gt; float:</code>,
meaning it must accept a <code>skplumber.Pipeline</code> object, dataset <code>X</code> with
corresponding target vector <code>y</code>, and an instantiation of a class
inheriting from <code><a title="skplumber.metrics.Metric" href="metrics.html#skplumber.metrics.Metric">Metric</a></code>. Should fit on the dataset
and compute and return its performance on that dataset. For basic k-fold
cross validation or train/test split strategies, see
<code><a title="skplumber.evaluators.make_kfold_evaluator" href="evaluators.html#skplumber.evaluators.make_kfold_evaluator">make_kfold_evaluator()</a></code> or
<code><a title="skplumber.evaluators.make_train_test_evaluator" href="evaluators.html#skplumber.evaluators.make_train_test_evaluator">make_train_test_evaluator()</a></code>.
If no evaluator is provided, a default train/test split evaluation strategy
will be used. <code>evaluator</code> will be used during both the sampling and
hyperparameter tuning stages, if <code>tune==True</code>.</dd>
<dt><strong><code>pipeline_timeout</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>If supplied, the maximum number of seconds to spend evaluating any
one pipeline. If a sampled pipeline takes longer than this to evaluate,
it will be skipped.</dd>
<dt><strong><code>exit_on_pipeline_error</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to exit if a specific pipeline errors out while training. If
<code>False</code>, the pipeline will just be skipped and <code><a title="skplumber.skplumber.SKPlumber" href="#skplumber.skplumber.SKPlumber">SKPlumber</a></code> will continue.</dd>
<dt><strong><code>callback</code></strong> :&ensp;<code>function</code>, optional</dt>
<dd>Optional callback function. Will be called after each sampled pipeline
is evaluated. Should have the function signature
<code>callback(state: SamplerState) -&gt; bool</code>. If <code>callback</code> returns <code>True</code>, the
sampling or hyperparameter optimization will end prematurely. <code>state</code>
is a named tuple containing these members:
- pipeline: The pipeline fitted in the previous iteration.
- score: The score of <code>pipeline</code>.
- train_time: The number of seconds it took to train and
evaluate <code>pipeline</code>.
- n_iters: The number of iterations completed so far.</dd>
<dt><strong><code>block_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The block size to take extrema from when using the block maxima approach
to calculate return times in the Generalized Extreme Value (GEV)
distribution fit to the pipeline sample results as the sampler progresses.</dd>
<dt><strong><code>tune</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to perform hyperparameter optimization on the best found pipeline.</dd>
<dt><strong><code>tuning_mult_factor</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Each hyperparameter tuning generation will have a population size equal to
the number of hyerparameters on the pipeline being optimized times this
value.</dd>
<dt><strong><code>log_level</code></strong> :&ensp;<code>{'INFO', 'DEBUG', 'NOTEST', 'WARNING', 'ERROR', 'CRITICAL'}</code>, optional</dt>
<dd>Log level SKPlumber should use while running. Defaults to <code>"INFO"</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SKPlumber:

    _models_map: t.Dict[ProblemType, t.List[t.Type[Primitive]]] = {
        ProblemType.CLASSIFICATION: list(classifiers.values()),
        ProblemType.REGRESSION: list(regressors.values()),
    }

    def __init__(
        self,
        problem: str,
        budget: int,
        *,
        metric: str = None,
        sampler: PipelineSampler = None,
        evaluator: t.Callable = None,
        pipeline_timeout: t.Optional[int] = None,
        exit_on_pipeline_error: bool = True,
        callback: t.Optional[t.Callable] = None,
        # TODO: make True by default. Requires being able to control the
        # amount of time it runs for (for tests and the budget)
        block_size: int = 10,
        tune: bool = False,
        tuning_mult_factor: int = 10,
        log_level: str = &#34;INFO&#34;,
    ) -&gt; None:
        &#34;&#34;&#34;
        Parameters
        ----------
        problem : str
            The type of problem you want to train this dataset for e.g.
            &#34;classification&#34;. See the values of the `skplumber.consts.ProblemType`
            enum for a list of valid options.
        budget : int
            How much time in seconds `fit` is allowed to search for a good solution.
        metric : Metric, optional
            The type of metric you want to score the pipeline predictions with.
            See the keys of the `skplumber.metrics.metrics` dict for a list of
            valid options. If `None`, a default metric will be used.
        sampler : PipelineSampler, optional
            An instance of a class inheriting from
            `skplumber.samplers.sampler.PipelineSampler`. Used to decide what
            types of pipelines to sample and try out on the problem. If `None`,
            the `skplumber.samplers.straight.StraightPipelineSampler` will be
            used with default arguments.
        evaluator : function, optional
            The evaluation strategy to use to fit and score a pipeline to determine
            its performance. Must be a function with signature:
            ```
            f(
                pipeline: Pipeline, X: pd.DataFrame, y: pd.Series, metric: Metric
            ) -&gt; float:
            ```,
            meaning it must accept a `skplumber.Pipeline` object, dataset `X` with
            corresponding target vector `y`, and an instantiation of a class
            inheriting from `skplumber.metrics.Metric`. Should fit on the dataset
            and compute and return its performance on that dataset. For basic k-fold
            cross validation or train/test split strategies, see
            `skplumber.evaluators.make_kfold_evaluator` or
            `skplumber.evaluators.make_train_test_evaluator`.
            If no evaluator is provided, a default train/test split evaluation strategy
            will be used. `evaluator` will be used during both the sampling and
            hyperparameter tuning stages, if `tune==True`.
        pipeline_timeout : int, optional
            If supplied, the maximum number of seconds to spend evaluating any
            one pipeline. If a sampled pipeline takes longer than this to evaluate,
            it will be skipped.
        exit_on_pipeline_error : bool, optional
            Whether to exit if a specific pipeline errors out while training. If
            `False`, the pipeline will just be skipped and `SKPlumber` will continue.
        callback : function, optional
            Optional callback function. Will be called after each sampled pipeline
            is evaluated. Should have the function signature
            `callback(state: SamplerState) -&gt; bool`. If `callback` returns `True`, the
            sampling or hyperparameter optimization will end prematurely. `state`
            is a named tuple containing these members:
                - pipeline: The pipeline fitted in the previous iteration.
                - score: The score of `pipeline`.
                - train_time: The number of seconds it took to train and
                  evaluate `pipeline`.
                - n_iters: The number of iterations completed so far.
        block_size : int, optional
            The block size to take extrema from when using the block maxima approach
            to calculate return times in the Generalized Extreme Value (GEV)
            distribution fit to the pipeline sample results as the sampler progresses.
        tune : bool, optional
            Whether to perform hyperparameter optimization on the best found pipeline.
        tuning_mult_factor : int, optional
            Each hyperparameter tuning generation will have a population size equal to
            the number of hyerparameters on the pipeline being optimized times this
            value.
        log_level : {&#39;INFO&#39;, &#39;DEBUG&#39;, &#39;NOTEST&#39;, &#39;WARNING&#39;, &#39;ERROR&#39;, &#39;CRITICAL&#39;}, optional
            Log level SKPlumber should use while running. Defaults to `&#34;INFO&#34;`.
        &#34;&#34;&#34;
        logger.setLevel(log_level)

        # Validate inputs

        self.problem_type = ProblemType(problem)

        valid_metric_names = [
            name
            for name, met in metrics.items()
            if met.problem_type == self.problem_type
        ]

        if metric is not None and metric not in valid_metric_names:
            raise ValueError(f&#34;metric is invalid, must be one of {valid_metric_names}&#34;)

        # Set defaults

        if metric is None:
            self.metric = default_metrics[self.problem_type]
        else:
            self.metric = metrics[metric]

        if sampler is None:
            self.sampler: PipelineSampler = StraightPipelineSampler()
        else:
            self.sampler = sampler

        if evaluator is None:
            self.evaluator = make_train_test_evaluator()
        else:
            self.evaluator = evaluator

        # Set other members

        self.budget = budget
        self.pipeline_timeout = pipeline_timeout
        self.tune = tune
        self.progress = EVProgress(block_size, self.metric.opt_dir)
        self.is_fitted = False
        self.tuning_mult_factor = tuning_mult_factor
        self.exit_on_pipeline_error = exit_on_pipeline_error

        self.sampler_cbs = [self._sampler_cb]
        if callback:
            self.sampler_cbs.append(callback)

    def fit(self, X: pd.DataFrame, y: pd.Series) -&gt; SearchResult:
        &#34;&#34;&#34;
        The main runtime method of the package. Given a dataset, problem type,
        and sampling strategy, it tries to find, in a limited amount of time,
        the best performing pipeline it can.

        Parameters
        ----------
        X : pandas.DataFrame
            The features of your dataset.
        y : pandas.Series
            The target vector of your dataset. The indices of `X` and `y`
            should match up.

        Returns
        -------
        result : SearchResult
            A named tuple containing data about how the fit process went.
        &#34;&#34;&#34;

        # Initialize

        if len(X.index) != y.size:
            raise ValueError(f&#34;X and y must have the same number of instances&#34;)

        # A little encapsulation to make this `fit` method&#39;s code less huge.
        self.state = SKPlumberFitState(self.budget, self.metric)

        # Run

        self.progress.start()
        best_pipeline, best_score, n_sample_iters = self.sampler.run(
            X,
            y,
            models=self._models_map[self.problem_type],
            transformers=list(transformers.values()),
            problem_type=self.problem_type,
            metric=self.metric,
            evaluator=self.evaluator,
            pipeline_timeout=self.pipeline_timeout,
            callback=self.sampler_cbs,
            exit_on_pipeline_error=self.exit_on_pipeline_error,
        )
        self.best_pipeline = best_pipeline
        self.state.best_score = best_score

        logger.info(f&#34;found best validation score of {best_score}&#34;)
        logger.info(&#34;best pipeline:&#34;)
        logger.info(self.best_pipeline)

        if self.tune:
            logger.info(
                &#34;now performing hyperparameter tuning on best found pipeline...&#34;
            )
            best_tuning_score, best_tuning_params, n_tune_iters = ga_tune(
                self.best_pipeline,
                X,
                y,
                self.evaluator,
                self.metric,
                self.exit_on_pipeline_error,
                population_size=(
                    self.best_pipeline.num_params * self.tuning_mult_factor
                ),
                callback=self._tuner_callback,
            )
            if self.metric.is_better_than(best_tuning_score, self.state.best_score):
                # The hyperparameter tuning was able to find an
                # improvement.
                self.state.best_score = best_tuning_score
                self.best_pipeline.set_params(best_tuning_params)
        else:
            n_tune_iters = 0

        # Now that we have the &#34;best&#34; model, train it on
        # the full dataset so it can see as much of the
        # dataset&#39;s distribution as possible in an effort
        # to be more ready for the wild.
        self.best_pipeline.fit(*shuffle(X, y))

        logger.info(
            &#34;finished. total execution time: &#34;
            f&#34;{time() - self.state.starttime:.2f} seconds.&#34;
        )
        logger.info(f&#34;final best score found: {self.state.best_score}&#34;)

        result = SearchResult(
            time() - self.state.starttime,
            n_sample_iters,
            n_tune_iters,
            self.state.best_score,
        )

        # Fitting completed successfully
        self.is_fitted = True
        return result

    def predict(self, X: pd.DataFrame) -&gt; pd.Series:
        &#34;&#34;&#34;
        Makes a prediction for each instance in `X`, returning the predictions.
        &#34;&#34;&#34;
        if not self.is_fitted:
            raise ValueError(
                &#34;`SKPlumber.fit` must be called and finish &#34;
                &#34;before `predict` can be called.&#34;
            )
        return self.best_pipeline.predict(X)

    def _sampler_cb(self, sampler_state: SamplerState) -&gt; bool:
        # Decide how much time is left available to us in
        # the sampling phase.
        if self.tune:
            # We want to leave enough time in the budget to be able
            # to complete at least one generation of hyperparameter tuning.
            if self.metric.is_better_than(sampler_state.score, self.state.best_score):
                self.state.best_score = sampler_state.score
                # An estimate of how long it will take to complete one
                # generation of hyperparameter tuning on this current
                # best pipeline.
                self.state.best_pipeline_min_tune_time = (
                    sampler_state.train_time
                    * sampler_state.pipeline.num_params
                    * self.tuning_mult_factor
                )
            sampling_endtime = (
                self.state.endbytime - self.state.best_pipeline_min_tune_time
            )
        else:
            sampling_endtime = self.state.endbytime

        now = time()
        logger.info(f&#34;{sampling_endtime - now:.2f} seconds left in sampling budget&#34;)

        # Logic for tracking sampler progress and exiting when the cost
        # of finding a new best score is too great.
        self.progress.observe(sampler_state.score)

        exit_early = False
        if now &gt; sampling_endtime:

            exit_early = True

        elif self.progress.can_report:

            logger.info(
                f&#34;estimated time to new best: {self.progress.return_time:.2f} seconds&#34;
            )
            if now + self.progress.return_time &gt; sampling_endtime:  # type: ignore
                exit_early = True

        if exit_early:
            logger.info(
                &#34;not enough time is left in the budget to find a new &#34;
                &#34;best score, so no more sampling will be done&#34;
            )

        return exit_early

    def _tuner_callback(self, tuner_state: dict) -&gt; bool:
        now = time()
        logger.info(
            f&#34;candidate pipeline in generation {tuner_state[&#39;nit&#39;]} finished. &#34;
            f&#34;{self.state.endbytime - now:.2f} seconds left in budget.&#34;
        )
        logger.info(f&#34;best score found so far: {tuner_state[&#39;fun&#39;]}&#34;)
        logger.info(
            f&#34;best hyperparameter config found so far: {tuner_state[&#39;kwargs_opt&#39;]}&#34;
        )
        # We need to quit early if our time budget is used up.
        return True if time() &gt; self.state.endbytime else False</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="skplumber.skplumber.SKPlumber.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X: pandas.core.frame.DataFrame, y: pandas.core.series.Series) -> <a title="skplumber.skplumber.SearchResult" href="#skplumber.skplumber.SearchResult">SearchResult</a></span>
</code></dt>
<dd>
<div class="desc"><p>The main runtime method of the package. Given a dataset, problem type,
and sampling strategy, it tries to find, in a limited amount of time,
the best performing pipeline it can.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>The features of your dataset.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>pandas.Series</code></dt>
<dd>The target vector of your dataset. The indices of <code>X</code> and <code>y</code>
should match up.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>result</code></strong> :&ensp;<code><a title="skplumber.skplumber.SearchResult" href="#skplumber.skplumber.SearchResult">SearchResult</a></code></dt>
<dd>A named tuple containing data about how the fit process went.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X: pd.DataFrame, y: pd.Series) -&gt; SearchResult:
    &#34;&#34;&#34;
    The main runtime method of the package. Given a dataset, problem type,
    and sampling strategy, it tries to find, in a limited amount of time,
    the best performing pipeline it can.

    Parameters
    ----------
    X : pandas.DataFrame
        The features of your dataset.
    y : pandas.Series
        The target vector of your dataset. The indices of `X` and `y`
        should match up.

    Returns
    -------
    result : SearchResult
        A named tuple containing data about how the fit process went.
    &#34;&#34;&#34;

    # Initialize

    if len(X.index) != y.size:
        raise ValueError(f&#34;X and y must have the same number of instances&#34;)

    # A little encapsulation to make this `fit` method&#39;s code less huge.
    self.state = SKPlumberFitState(self.budget, self.metric)

    # Run

    self.progress.start()
    best_pipeline, best_score, n_sample_iters = self.sampler.run(
        X,
        y,
        models=self._models_map[self.problem_type],
        transformers=list(transformers.values()),
        problem_type=self.problem_type,
        metric=self.metric,
        evaluator=self.evaluator,
        pipeline_timeout=self.pipeline_timeout,
        callback=self.sampler_cbs,
        exit_on_pipeline_error=self.exit_on_pipeline_error,
    )
    self.best_pipeline = best_pipeline
    self.state.best_score = best_score

    logger.info(f&#34;found best validation score of {best_score}&#34;)
    logger.info(&#34;best pipeline:&#34;)
    logger.info(self.best_pipeline)

    if self.tune:
        logger.info(
            &#34;now performing hyperparameter tuning on best found pipeline...&#34;
        )
        best_tuning_score, best_tuning_params, n_tune_iters = ga_tune(
            self.best_pipeline,
            X,
            y,
            self.evaluator,
            self.metric,
            self.exit_on_pipeline_error,
            population_size=(
                self.best_pipeline.num_params * self.tuning_mult_factor
            ),
            callback=self._tuner_callback,
        )
        if self.metric.is_better_than(best_tuning_score, self.state.best_score):
            # The hyperparameter tuning was able to find an
            # improvement.
            self.state.best_score = best_tuning_score
            self.best_pipeline.set_params(best_tuning_params)
    else:
        n_tune_iters = 0

    # Now that we have the &#34;best&#34; model, train it on
    # the full dataset so it can see as much of the
    # dataset&#39;s distribution as possible in an effort
    # to be more ready for the wild.
    self.best_pipeline.fit(*shuffle(X, y))

    logger.info(
        &#34;finished. total execution time: &#34;
        f&#34;{time() - self.state.starttime:.2f} seconds.&#34;
    )
    logger.info(f&#34;final best score found: {self.state.best_score}&#34;)

    result = SearchResult(
        time() - self.state.starttime,
        n_sample_iters,
        n_tune_iters,
        self.state.best_score,
    )

    # Fitting completed successfully
    self.is_fitted = True
    return result</code></pre>
</details>
</dd>
<dt id="skplumber.skplumber.SKPlumber.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X: pandas.core.frame.DataFrame) -> pandas.core.series.Series</span>
</code></dt>
<dd>
<div class="desc"><p>Makes a prediction for each instance in <code>X</code>, returning the predictions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X: pd.DataFrame) -&gt; pd.Series:
    &#34;&#34;&#34;
    Makes a prediction for each instance in `X`, returning the predictions.
    &#34;&#34;&#34;
    if not self.is_fitted:
        raise ValueError(
            &#34;`SKPlumber.fit` must be called and finish &#34;
            &#34;before `predict` can be called.&#34;
        )
    return self.best_pipeline.predict(X)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="skplumber.skplumber.SKPlumberFitState"><code class="flex name class">
<span>class <span class="ident">SKPlumberFitState</span></span>
<span>(</span><span>budget: int, metric: <a title="skplumber.metrics.Metric" href="metrics.html#skplumber.metrics.Metric">Metric</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SKPlumberFitState:
    def __init__(self, budget: int, metric: Metric) -&gt; None:
        self.starttime = time()
        self.endbytime = self.starttime + budget
        self.best_pipeline_min_tune_time = 0.0
        self.best_score = metric.worst_value</code></pre>
</details>
</dd>
<dt id="skplumber.skplumber.SearchResult"><code class="flex name class">
<span>class <span class="ident">SearchResult</span></span>
<span>(</span><span>time: float, n_sample_iters: int, n_tune_iters: int, best_score: float)</span>
</code></dt>
<dd>
<div class="desc"><p>SearchResult(time, n_sample_iters, n_tune_iters, best_score)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SearchResult(NamedTuple):
    # total train time in seconds
    time: float
    # total number of pipelines the sampler tried
    n_sample_iters: int
    # total number of pipelines the hyperparameter tuner tried
    n_tune_iters: int
    # the best score SKPlumber was able to find
    best_score: float</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="skplumber.skplumber.SearchResult.best_score"><code class="name">var <span class="ident">best_score</span> : float</code></dt>
<dd>
<div class="desc"><p>Alias for field number 3</p></div>
</dd>
<dt id="skplumber.skplumber.SearchResult.n_sample_iters"><code class="name">var <span class="ident">n_sample_iters</span> : int</code></dt>
<dd>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
<dt id="skplumber.skplumber.SearchResult.n_tune_iters"><code class="name">var <span class="ident">n_tune_iters</span> : int</code></dt>
<dd>
<div class="desc"><p>Alias for field number 2</p></div>
</dd>
<dt id="skplumber.skplumber.SearchResult.time"><code class="name">var <span class="ident">time</span> : float</code></dt>
<dd>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="skplumber" href="index.html">skplumber</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="skplumber.skplumber.SKPlumber" href="#skplumber.skplumber.SKPlumber">SKPlumber</a></code></h4>
<ul class="">
<li><code><a title="skplumber.skplumber.SKPlumber.fit" href="#skplumber.skplumber.SKPlumber.fit">fit</a></code></li>
<li><code><a title="skplumber.skplumber.SKPlumber.predict" href="#skplumber.skplumber.SKPlumber.predict">predict</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="skplumber.skplumber.SKPlumberFitState" href="#skplumber.skplumber.SKPlumberFitState">SKPlumberFitState</a></code></h4>
</li>
<li>
<h4><code><a title="skplumber.skplumber.SearchResult" href="#skplumber.skplumber.SearchResult">SearchResult</a></code></h4>
<ul class="">
<li><code><a title="skplumber.skplumber.SearchResult.best_score" href="#skplumber.skplumber.SearchResult.best_score">best_score</a></code></li>
<li><code><a title="skplumber.skplumber.SearchResult.n_sample_iters" href="#skplumber.skplumber.SearchResult.n_sample_iters">n_sample_iters</a></code></li>
<li><code><a title="skplumber.skplumber.SearchResult.n_tune_iters" href="#skplumber.skplumber.SearchResult.n_tune_iters">n_tune_iters</a></code></li>
<li><code><a title="skplumber.skplumber.SearchResult.time" href="#skplumber.skplumber.SearchResult.time">time</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>